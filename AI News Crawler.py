#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Crawler - è‡ªåŠ¨åŒ–AIé¢†åŸŸæ–°é—»èšåˆä¸æ‘˜è¦ç”Ÿæˆå·¥å…·

åŠŸèƒ½æ¦‚è¿°ï¼š
- ä»å¤šä¸ªAIæƒå¨æ•°æ®æºè·å–æœ€æ–°æ–°é—»å’Œè®ºæ–‡
- è°ƒç”¨é˜¿é‡Œç™¾ç‚¼APIè¿›è¡Œæ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
- ç”Ÿæˆç¬¦åˆJekyllæ ‡å‡†çš„Markdownæ ¼å¼æ–‡ç« 
- è‡ªåŠ¨ä¿å­˜åˆ°_postsç›®å½•ä¾›Jekyllåšå®¢ä½¿ç”¨
- æ”¯æŒä»£ç†é…ç½®ã€è¯·æ±‚é™æµã€è‡ªåŠ¨é‡è¯•ç­‰é«˜çº§åŠŸèƒ½

æ”¯æŒçš„æ•°æ®æºï¼š
- ArXiv AIç›¸å…³åˆ†ç±»ï¼šcs.AI, cs.LG, cs.CL, cs.CV
- Google DeepMind Blog
- OpenAI Blog
- TechCrunch AI/ML
- MIT Technology Review
- Wired AIæŠ¥é“
- The Verge AIæ–°é—»
- æœºå™¨ä¹‹å¿ƒï¼ˆä¸­æ–‡AIèµ„è®¯ï¼‰

å®‰è£…ä¾èµ–ï¼š
pip install requests feedparser beautifulsoup4 pyyaml

é…ç½®æ–‡ä»¶ç¤ºä¾‹ (config.yaml)ï¼š
# config.yaml
ali_baillan:
  api_key: "YOUR_ALI_BAILLAN_API_KEY"
  endpoint: "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
  
rss_sources:
  - url: "https://rss.arxiv.org/rss/cs.AI"
    name: "ArXiv AI"
    type: "academic"
  - url: "https://deepmind.google/blog/rss.xml"
    name: "Google DeepMind"
    type: "blog"
  - url: "https://openai.com/blog/rss.xml"
    name: "OpenAI"
    type: "blog"
  
crawler:
  max_articles: 20
  days_back: 7
  output_dir: "./_posts"
  log_level: "INFO"

ä½¿ç”¨æ–¹å¼ï¼š
python ai_news_crawler.py --config config.yaml
"""

import requests
import feedparser
from bs4 import BeautifulSoup
import yaml
import os
import json
import hashlib
import logging
import re
from datetime import datetime, timedelta
import time
import argparse
import sys
import signal
import html
import random
from typing import List, Dict, Optional, Any


# ==========================================
# 1. æ—¥å¿—ç®¡ç†å™¨
# ==========================================
class LoggerManager:
    """é«˜çº§æ—¥å¿—ç®¡ç†å™¨ï¼Œæ”¯æŒæ—¥å¿—è½®è½¬å’Œæ ¼å¼åŒ–"""

    @staticmethod
    def setup_logging(log_level="INFO", log_file="crawler.log"):
        """
        é…ç½®æ—¥å¿—ç³»ç»Ÿ
        
        Args:
            log_level (str): æ—¥å¿—çº§åˆ«
            log_file (str): æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        numeric_level = getattr(logging, log_level.upper(), None)
        if not isinstance(numeric_level, int):
            numeric_level = logging.INFO
        
        # å®šä¹‰æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        console_handler.setLevel(numeric_level)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶è®°å½•æ›´è¯¦ç»†çš„æ—¥å¿—
        
        # é…ç½®æ ¹æ—¥å¿—å™¨
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        root_logger.addHandler(console_handler)
        root_logger.addHandler(file_handler)
        
        # è®¾ç½®ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ï¼Œé¿å…è¿‡äºå˜ˆæ‚
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        logging.getLogger("requests").setLevel(logging.WARNING)


# ==========================================
# 2. æ–‡æœ¬è§„èŒƒåŒ–å·¥å…·
# ==========================================
class TextNormalizer:
    """æ–‡æœ¬å¤„ç†å·¥å…·ç±»ï¼Œç”¨äºæ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬"""

    @staticmethod
    def clean_html(raw_html):
        """
        ç§»é™¤HTMLæ ‡ç­¾å¹¶æ¸…ç†æ–‡æœ¬
        
        Args:
            raw_html (str): åŸå§‹HTMLå­—ç¬¦ä¸²
            
        Returns:
            str: æ¸…ç†åçš„çº¯æ–‡æœ¬
        """
        if not raw_html:
            return ""
        
        # è§£ç HTMLå®ä½“ï¼ˆå¦‚ & -> &ï¼‰
        decoded_html = html.unescape(raw_html)
        
        # ä½¿ç”¨BeautifulSoupç§»é™¤æ ‡ç­¾
        soup = BeautifulSoup(decoded_html, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "iframe"]):
            script.decompose()
        
        # è·å–æ–‡æœ¬
        text = soup.get_text(separator=' ')
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    @staticmethod
    def normalize_whitespace(text):
        """è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦"""
        return re.sub(r'[ \t\r\n]+', ' ', text).strip()

    @staticmethod
    def truncate_text(text, max_length=5000, suffix="..."):
        """æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬"""
        if len(text) <= max_length:
            return text
        return text[:max_length].rsplit(' ', 1)[0] + suffix


# ==========================================
# 3. é™æµå™¨
# ==========================================
class RateLimiter:
    """ç®€å•çš„è¯·æ±‚é™æµå™¨ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹"""

    def __init__(self, min_interval=1.0):
        """
        åˆå§‹åŒ–é™æµå™¨
        
        Args:
            min_interval (float): ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰
        """
        self.min_interval = min_interval
        self.last_called = 0

    def wait(self):
        """ç­‰å¾…ç›´åˆ°æ»¡è¶³æœ€å°é—´éš”"""
        current_time = time.time()
        elapsed = current_time - self.last_called
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            time.sleep(sleep_time)
        self.last_called = time.time()


# ==========================================
# 4. é…ç½®ç®¡ç†å™¨
# ==========================================
class ConfigManager:
    """é…ç½®ç®¡ç†ç±»ï¼Œè´Ÿè´£è¯»å–ã€éªŒè¯å’Œæä¾›é…ç½®"""

    def __init__(self, config_path):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = {}
        self.load_config()
        self.validate_config()
        self._set_defaults()
    
    def load_config(self):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logging.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
        except FileNotFoundError:
            logging.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
            raise
        except yaml.YAMLError as e:
            logging.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
    
    def validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶çš„å¿…è¦å­—æ®µ"""
        if not self.config:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸ºç©º")
            
        required_sections = ['ali_baillan', 'rss_sources', 'crawler']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {section}")
        
        # éªŒè¯é˜¿é‡Œç™¾ç‚¼é…ç½®
        ali_config = self.config['ali_baillan']
        if not ali_config.get('api_key'):
            raise ValueError("é˜¿é‡Œç™¾ç‚¼é…ç½®ç¼ºå°‘api_key")
        if not ali_config.get('endpoint'):
            # è®¾ç½®é»˜è®¤endpoint
            ali_config['endpoint'] = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
            logging.info("ä½¿ç”¨é»˜è®¤çš„é˜¿é‡Œç™¾ç‚¼API endpoint")
        
        # éªŒè¯RSSæºé…ç½®
        if not isinstance(self.config['rss_sources'], list):
            raise ValueError("rss_sourceså¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼")
        if len(self.config['rss_sources']) == 0:
            raise ValueError("rss_sourcesåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            
        for source in self.config['rss_sources']:
            if 'url' not in source or 'name' not in source:
                raise ValueError("æ¯ä¸ªRSSæºå¿…é¡»åŒ…å«urlå’Œnameå­—æ®µ")
        
        # éªŒè¯çˆ¬è™«é…ç½®
        crawler_config = self.config['crawler']
        required_crawler_fields = ['max_articles', 'days_back', 'output_dir']
        for field in required_crawler_fields:
            if field not in crawler_config:
                raise ValueError(f"çˆ¬è™«é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
    
    def _set_defaults(self):
        """è®¾ç½®å¯é€‰å­—æ®µçš„é»˜è®¤å€¼"""
        crawler_config = self.config['crawler']
        
        if 'timeout' not in crawler_config:
            crawler_config['timeout'] = 10
        if 'retries' not in crawler_config:
            crawler_config['retries'] = 3
        if 'user_agent' not in crawler_config:
            crawler_config['user_agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        if 'request_delay' not in crawler_config:
            crawler_config['request_delay'] = 1.0

    def get_ali_baillan_config(self):
        """è·å–é˜¿é‡Œç™¾ç‚¼APIé…ç½®"""
        return self.config['ali_baillan']
    
    def get_rss_sources(self):
        """è·å–RSSæºåˆ—è¡¨"""
        return self.config['rss_sources']
    
    def get_crawler_config(self):
        """è·å–çˆ¬è™«é…ç½®"""
        return self.config['crawler']


# ==========================================
# 5. RSSè§£æå™¨
# ==========================================
class RSSParser:
    """RSS/Atomæºè§£æå™¨"""

    def __init__(self, days_back=7, timeout=20):
        """
        åˆå§‹åŒ–RSSè§£æå™¨
        
        Args:
            days_back (int): è·å–å¤šå°‘å¤©å†…çš„æ–‡ç« 
            timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.days_back = days_back
        self.cutoff_date = datetime.now() - timedelta(days=days_back)
        self.timeout = timeout
    
    def parse_feed(self, feed_url, source_name, source_type):
        """
        è§£æå•ä¸ªRSS/Atomæº
        
        Args:
            feed_url (str): RSSæºURL
            source_name (str): æºåç§°
            source_type (str): æºç±»å‹
            
        Returns:
            list: æ–‡ç« åˆ—è¡¨
        """
        articles = []
        try:
            logging.info(f"æ­£åœ¨è§£æRSSæº: {source_name} ({feed_url})")
            
            # ä½¿ç”¨User-Agenté¿å…è¢«æŸäº›æºæ‹’ç»
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; AI-News-Crawler/1.0)'}
            
            feed = feedparser.parse(feed_url, request_headers=headers)
            
            if feed.bozo:
                # å³ä½¿æœ‰è­¦å‘Šï¼Œä¹Ÿå°è¯•ç»§ç»­è§£æ
                logging.warning(f"RSSæºè§£æè­¦å‘Š: {source_name} - {feed.bozo_exception}")
            
            if not feed.entries:
                logging.warning(f"RSSæº {source_name} æ²¡æœ‰è·å–åˆ°ä»»ä½•æ¡ç›®")
                return []
            
            for entry in feed.entries:
                # æå–å‘å¸ƒæ—¶é—´
                published = self._extract_published_date(entry)
                
                # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œæˆ–è€…æ—¶é—´å¤ªæ—§ï¼Œåˆ™è·³è¿‡ï¼ˆé™¤éæ˜¯å¼ºåˆ¶æŠ“å–ï¼‰
                if published and published < self.cutoff_date:
                    continue
                
                # æå–æ–‡ç« ä¿¡æ¯
                article = {
                    'title': self._clean_text(entry.get('title', '')),
                    'link': self._extract_link(entry),
                    'published': published or datetime.now(), # é»˜è®¤ä¸ºå½“å‰æ—¶é—´
                    'summary': self._clean_text(entry.get('summary', entry.get('description', ''))),
                    'source_name': source_name,
                    'source_type': source_type,
                    'raw_content': self._extract_content(entry),
                    'tags': self._extract_tags(entry)
                }
                
                # å¦‚æœsummaryä¸ºç©ºï¼Œå°è¯•ä»contentæå–
                if not article['summary'] and article['raw_content']:
                    article['summary'] = TextNormalizer.clean_html(article['raw_content'])[:300]
                
                # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡
                if not article['title']:
                    continue
                    
                articles.append(article)
            
            logging.info(f"ä» {source_name} è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            return articles
            
        except Exception as e:
            logging.error(f"è§£æRSSæºå¤±è´¥ {source_name}: {e}", exc_info=True)
            return []
    
    def _extract_link(self, entry):
        """æå–é“¾æ¥ï¼Œå¤„ç†æŸäº›RSSæºä¸­é“¾æ¥åœ¨hrefå±æ€§çš„æƒ…å†µ"""
        link = entry.get('link')
        if not link:
            # å°è¯•ä»linksæ•°ç»„ä¸­æ‰¾
            links = entry.get('links', [])
            if links:
                link = links[0].get('href')
        return link if link else ""

    def _extract_published_date(self, entry):
        """ä»RSSæ¡ç›®ä¸­æå–å‘å¸ƒæ—¶é—´"""
        # ä¼˜å…ˆè§£æç»“æ„åŒ–æ—¶é—´
        time_fields = ['published_parsed', 'updated_parsed']
        for field in time_fields:
            if hasattr(entry, field) and getattr(entry, field):
                parsed_time = getattr(entry, field)
                try:
                    return datetime(*parsed_time[:6])
                except (TypeError, ValueError):
                    pass
        
        # å°è¯•è§£æå­—ç¬¦ä¸²æ—¶é—´
        str_fields = ['published', 'updated', 'created', 'date']
        for field in str_fields:
            if hasattr(entry, field) and getattr(entry, field):
                date_str = getattr(entry, field)
                # å°è¯•å¸¸è§æ ¼å¼
                formats = [
                    '%Y-%m-%dT%H:%M:%SZ',
                    '%Y-%m-%dT%H:%M:%S%z',
                    '%Y-%m-%d %H:%M:%S',
                    '%a, %d %b %Y %H:%M:%S %Z',
                    '%a, %d %b %Y %H:%M:%S %z'
                ]
                for fmt in formats:
                    try:
                        return datetime.strptime(date_str, fmt)
                    except ValueError:
                        continue
        return None

    def _extract_content(self, entry):
        """æå–æ–‡ç« å†…å®¹"""
        if 'content' in entry and entry['content']:
            return entry['content'][0].get('value', '')
        if 'summary' in entry and entry['summary']:
            return entry['summary']
        return ""

    def _extract_tags(self, entry):
        """æå–æ ‡ç­¾"""
        tags = []
        if 'tags' in entry:
            tags = [tag.get('term') for tag in entry['tags'] if tag.get('term')]
        return tags

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        return TextNormalizer.clean_html(text)


# ==========================================
# 6. å†…å®¹ä¸‹è½½å™¨
# ==========================================
class ContentDownloader:
    """æ–‡ç« å†…å®¹ä¸‹è½½å™¨ï¼Œæ”¯æŒé‡è¯•å’Œä»£ç†"""

    def __init__(self, timeout=10, retries=3, user_agent=None):
        """
        åˆå§‹åŒ–å†…å®¹ä¸‹è½½å™¨
        
        Args:
            timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´
            retries (int): é‡è¯•æ¬¡æ•°
            user_agent (str): User-Agentå­—ç¬¦ä¸²
        """
        self.timeout = timeout
        self.retries = retries
        self.session = requests.Session()
        
        headers = {
            'User-Agent': user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        self.session.headers.update(headers)
    
    def download_content(self, url):
        """
        ä¸‹è½½æ–‡ç« å®Œæ•´å†…å®¹
        
        Args:
            url (str): æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« ä¸»è¦å†…å®¹æ–‡æœ¬
        """
        for attempt in range(self.retries):
            try:
                logging.debug(f"æ­£åœ¨ä¸‹è½½æ–‡ç« å†…å®¹: {url} (å°è¯• {attempt + 1}/{self.retries})")
                
                response = self.session.get(url, timeout=self.timeout, allow_redirects=True)
                response.raise_for_status()
                
                # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = response.apparent_encoding
                
                return self._extract_main_content(response.text, url)
                
            except requests.exceptions.SSLError:
                logging.warning(f"SSLé”™è¯¯ï¼Œå°è¯•å¿½ç•¥éªŒè¯: {url}")
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ verify=Falseçš„é€»è¾‘ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œé€šå¸¸ä¸æ¨è
                return ""
            except requests.exceptions.Timeout:
                logging.warning(f"è¯·æ±‚è¶…æ—¶: {url}")
                if attempt < self.retries - 1:
                    time.sleep(2 ** attempt)
            except requests.exceptions.RequestException as e:
                logging.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}): {url} - {e}")
                if attempt < self.retries - 1:
                    time.sleep(2 ** attempt)
            except Exception as e:
                logging.error(f"æœªçŸ¥é”™è¯¯: {url} - {e}")
                break
        
        return ""

    def _extract_main_content(self, html, url):
        """
        ä»HTMLä¸­æå–ä¸»è¦å†…å®¹
        
        Args:
            html (str): HTMLå­—ç¬¦ä¸²
            url (str): é¡µé¢URLï¼Œç”¨äºç‰¹å®šç½‘ç«™çš„å®šåˆ¶è§£æ
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç§»é™¤å¹²æ‰°å…ƒç´ 
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "iframe", "noscript"]):
            element.decompose()
        
        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„å®šåˆ¶è§£æé€»è¾‘
        main_content = None
        
        # ArXiv ç‰¹æ®Šå¤„ç†
        if 'arxiv.org' in url:
            main_content = soup.find('div', class_='ltx_page_main')
        
        # OpenAI Blog ç‰¹æ®Šå¤„ç†
        elif 'openai.com' in url:
            main_content = soup.find('div', class_='f-body-1')
            
        # é€šç”¨ç­–ç•¥ï¼šå¯»æ‰¾å¸¸è§çš„æ–‡ç« å®¹å™¨
        if not main_content:
            selectors = [
                'article',
                'main',
                '[role="main"]',
                '.post-content',
                '.entry-content',
                '.article-content',
                '.content',
                '#content',
                '.post-body',
                '.markdown-body'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    # é€‰æ‹©å†…å®¹æœ€é•¿çš„é‚£ä¸ªå®¹å™¨
                    main_content = max(elements, key=lambda e: len(e.get_text()))
                    break
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨body
        if not main_content:
            main_content = soup.find('body')
        
        if main_content:
            text = main_content.get_text(separator='\n')
            text = TextNormalizer.normalize_whitespace(text)
            # é™åˆ¶é•¿åº¦ï¼Œé¿å…å¤„ç†è¿‡æ…¢æˆ–è¶…å‡ºAPIé™åˆ¶
            return TextNormalizer.truncate_text(text, max_length=8000)
        
        return ""


# ==========================================
# 7. é˜¿é‡Œç™¾ç‚¼APIå®¢æˆ·ç«¯
# ==========================================
class AliBaillanAPIClient:
    """é˜¿é‡Œç™¾ç‚¼APIå®¢æˆ·ç«¯ï¼Œç”¨äºç”Ÿæˆæ‘˜è¦"""

    def __init__(self, api_key, endpoint):
        """
        åˆå§‹åŒ–é˜¿é‡Œç™¾ç‚¼APIå®¢æˆ·ç«¯
        
        Args:
            api_key (str): APIå¯†é’¥
            endpoint (str): APIç«¯ç‚¹URL
        """
        self.api_key = api_key
        self.endpoint = endpoint
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })
    
    def generate_summary(self, content, title="", max_tokens=500):
        """
        è°ƒç”¨é˜¿é‡Œç™¾ç‚¼APIç”Ÿæˆå†…å®¹æ‘˜è¦
        
        Args:
            content (str): åŸå§‹å†…å®¹
            title (str): æ–‡ç« æ ‡é¢˜
            max_tokens (int): æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬
        """
        if not content or len(content.strip()) < 50:
            return "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦ã€‚"
        
        # æˆªæ–­è¿‡é•¿çš„å†…å®¹è¾“å…¥ï¼ŒèŠ‚çœToken
        input_content = TextNormalizer.truncate_text(content, 4000)
        
        # æ„å»ºæç¤ºè¯
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘æŠ€æ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿å°†é•¿ç¯‡æ–‡ç« æ€»ç»“ä¸ºç²¾ç‚¼çš„ä¸­æ–‡æ‘˜è¦ã€‚"
        user_prompt = f"è¯·é˜…è¯»ä»¥ä¸‹æ–‡ç« ï¼Œå¹¶ç”Ÿæˆä¸€æ®µ200-300å­—çš„ä¸­æ–‡æ‘˜è¦ã€‚æ‘˜è¦åº”åŒ…å«æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ã€‚\n\næ ‡é¢˜ï¼š{title}\n\næ­£æ–‡ï¼š\n{input_content}"
        
        payload = {
            "model": "qwen-max",  # ä½¿ç”¨é€šä¹‰åƒé—®æœ€å¤§æ¨¡å‹
            "input": {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            },
            "parameters": {
                "max_tokens": max_tokens,
                "temperature": 0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„æ‘˜è¦
                "top_p": 0.8
            }
        }
        
        try:
            logging.info(f"æ­£åœ¨è°ƒç”¨é˜¿é‡Œç™¾ç‚¼APIç”Ÿæˆæ‘˜è¦ (æ ‡é¢˜: {title[:20]}...)")
            response = self.session.post(
                self.endpoint,
                json=payload,
                timeout=60  # APIè°ƒç”¨å¯èƒ½è¾ƒæ…¢
            )
            response.raise_for_status()
            
            result = response.json()
            
            # æ£€æŸ¥APIè¿”å›çŠ¶æ€
            if result.get('code') and result['code'] != 'Success':
                error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
                logging.error(f"APIè¿”å›é”™è¯¯: {error_msg}")
                return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šAPIé”™è¯¯ - {error_msg}"
            
            if 'output' in result and 'text' in result['output']:
                summary = result['output']['text'].strip()
                # æ¸…ç†å¯èƒ½å‡ºç°çš„markdownæ ‡è®°
                summary = re.sub(r'^#+\s*', '', summary)
                logging.info("æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                return summary
            else:
                logging.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼å¼‚å¸¸"
                
        except requests.exceptions.Timeout:
            logging.error("APIè°ƒç”¨è¶…æ—¶")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šè¯·æ±‚è¶…æ—¶"
        except requests.exceptions.RequestException as e:
            logging.error(f"APIè°ƒç”¨ç½‘ç»œé”™è¯¯: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šç½‘ç»œé”™è¯¯"
        except Exception as e:
            logging.error(f"æ‘˜è¦ç”Ÿæˆå¼‚å¸¸: {e}", exc_info=True)
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"


# ==========================================
# 8. Markdownç”Ÿæˆå™¨
# ==========================================
class MarkdownGenerator:
    """Markdownæ–‡ä»¶ç”Ÿæˆå™¨ï¼Œç”ŸæˆJekyllå…¼å®¹æ ¼å¼"""

    def __init__(self, output_dir="./_posts"):
        """
        åˆå§‹åŒ–Markdownç”Ÿæˆå™¨
        
        Args:
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.ensure_output_dir()
    
    def ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            try:
                os.makedirs(self.output_dir)
                logging.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {self.output_dir}")
            except OSError as e:
                logging.error(f"æ— æ³•åˆ›å»ºç›®å½• {self.output_dir}: {e}")
                raise
    
    def generate_filename(self, title, date):
        """
        ç”ŸæˆMarkdownæ–‡ä»¶å (Jekyllæ ¼å¼: YYYY-MM-DD-title.md)
        
        Args:
            title (str): æ–‡ç« æ ‡é¢˜
            date (datetime): å‘å¸ƒæ—¥æœŸ
            
        Returns:
            str: æ–‡ä»¶å
        """
        # è½¬æ¢æ ‡é¢˜ä¸ºå®‰å…¨çš„æ–‡ä»¶å
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        clean_title = title.lower()
        clean_title = re.sub(r'[^\w\s-]', '', clean_title)  # ç§»é™¤éå•è¯å­—ç¬¦
        clean_title = re.sub(r'[-\s]+', '-', clean_title).strip('-')  # æ›¿æ¢ç©ºæ ¼å’Œè¿å­—ç¬¦
        
        # é™åˆ¶é•¿åº¦
        clean_title = clean_title[:80]
        
        date_str = date.strftime('%Y-%m-%d')
        return f"{date_str}-{clean_title}"
    
    def generate_front_matter(self, article_data):
        """
        ç”ŸæˆJekyll Front Matter
        
        Args:
            article_data (dict): åŒ…å«title, date, categories, tags, source_nameç­‰
            
        Returns:
            str: Front Matterå­—ç¬¦ä¸²
        """
        date = article_data.get('published', datetime.now())
        # Jekyllæ—¥æœŸæ ¼å¼é€šå¸¸åŒ…å«æ—¶åŒº
        date_str = date.strftime('%Y-%m-%d %H:%M:%S +0800')
        
        title = article_data.get('title', 'Untitled')
        # è½¬ä¹‰YAMLä¸­çš„åŒå¼•å·
        title = title.replace('"', '\\"')
        
        categories = article_data.get('categories', ['AIæ–°é—»'])
        tags = article_data.get('tags', [])
        
        # æ·»åŠ æ¥æºä½œä¸ºæ ‡ç­¾
        source = article_data.get('source_name', 'Unknown')
        if source not in tags:
            tags.append(source)
            
        front_matter = "---\n"
        front_matter += f"layout: post\n"
        front_matter += f"title: \"{title}\"\n"
        front_matter += f"date: {date_str}\n"
        
        # å¤„ç†åˆ—è¡¨ç±»å‹çš„YAML
        front_matter += f"categories: {json.dumps(categories, ensure_ascii=False)}\n"
        front_matter += f"tags: {json.dumps(tags, ensure_ascii=False)}\n"
        
        # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
        front_matter += f"source: {source}\n"
        front_matter += f"source_url: {article_data.get('link', '')}\n"
        
        front_matter += "---\n\n"
        
        return front_matter
    
    def generate_markdown_content(self, article_data, ai_summary, original_content):
        """
        ç”Ÿæˆå®Œæ•´çš„Markdownå†…å®¹
        
        Args:
            article_data (dict): æ–‡ç« å…ƒæ•°æ®
            ai_summary (str): AIç”Ÿæˆçš„æ‘˜è¦
            original_content (str): åŸæ–‡å†…å®¹
            
        Returns:
            str: å®Œæ•´çš„Markdownå†…å®¹
        """
        # ç”ŸæˆFront Matter
        front_matter = self.generate_front_matter(article_data)
        
        # ç”Ÿæˆæ­£æ–‡
        content = front_matter
        
        # æ·»åŠ AIæ‘˜è¦éƒ¨åˆ†
        content += "## ğŸ¤– AI æ‘˜è¦\n\n"
        content += f"{ai_summary}\n\n"
        content += "---\n\n"
        
        # æ·»åŠ åŸæ–‡éƒ¨åˆ†
        content += "## ğŸ“„ åŸæ–‡æ¦‚è¦\n\n"
        
        # å¦‚æœåŸæ–‡å¤ªé•¿ï¼Œåªæ˜¾ç¤ºä¸€éƒ¨åˆ†
        preview_length = 2000
        if len(original_content) > preview_length:
            content += original_content[:preview_length]
            content += f"\n\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼Œå…± {len(original_content)} å­—) ...\n\n"
        else:
            content += original_content + "\n\n"
            
        # æ·»åŠ åŸæ–‡é“¾æ¥
        content += "## ğŸ”— é˜…è¯»åŸæ–‡\n\n"
        content += f"[ç‚¹å‡»æŸ¥çœ‹åŸæ–‡]({article_data.get('link', '#')})\n"
        
        return content


# ==========================================
# 9. å»é‡æ£€æŸ¥å™¨
# ==========================================
class DuplicateChecker:
    """å†…å®¹å»é‡æ£€æŸ¥å™¨ï¼ŒåŸºäºMD5å“ˆå¸Œå’ŒURL"""

    def __init__(self, cache_file="crawler_cache.json"):
        """
        åˆå§‹åŒ–å»é‡æ£€æŸ¥å™¨
        
        Args:
            cache_file (str): ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        self.cache_file = cache_file
        self.cache = self.load_cache()
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logging.warning(f"ç¼“å­˜æ–‡ä»¶åŠ è½½å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜")
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except IOError as e:
            logging.error(f"ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    def get_article_hash(self, article):
        """
        è®¡ç®—æ–‡ç« çš„å”¯ä¸€æ ‡è¯†å“ˆå¸Œ
        ç»“åˆæ ‡é¢˜å’Œé“¾æ¥ï¼Œå› ä¸ºåŒä¸€ç¯‡æ–‡ç« å¯èƒ½æœ‰ä¸åŒçš„URLå‚æ•°ï¼Œæˆ–è€…åŒä¸€URLæ ‡é¢˜å˜äº†
        """
        title = article.get('title', '')
        link = article.get('link', '')
        # è§„èŒƒåŒ–URLï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        clean_link = re.sub(r'[?#].*$', '', link)
        
        identifier = f"{title}|{clean_link}"
        return hashlib.md5(identifier.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, article):
        """
        æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å¤„ç†
        
        Args:
            article (dict): æ–‡ç« å¯¹è±¡
            
        Returns:
            bool: æ˜¯å¦é‡å¤
        """
        article_hash = self.get_article_hash(article)
        return article_hash in self.cache
    
    def mark_as_processed(self, article):
        """
        æ ‡è®°æ–‡ç« ä¸ºå·²å¤„ç†
        
        Args:
            article (dict): æ–‡ç« å¯¹è±¡
        """
        article_hash = self.get_article_hash(article)
        self.cache[article_hash] = {
            'title': article.get('title'),
            'link': article.get('link'),
            'processed_at': datetime.now().isoformat(),
            'source': article.get('source_name')
        }
        # æ¯æ¬¡æ›´æ–°éƒ½ä¿å­˜ï¼Œé˜²æ­¢ç¨‹åºå´©æºƒå¯¼è‡´ä¸¢å¤±
        self.save_cache()


# ==========================================
# 10. æ–‡ä»¶ç®¡ç†å™¨
# ==========================================
class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨ï¼Œè´Ÿè´£æ–‡ä»¶çš„å®‰å…¨å†™å…¥"""

    def __init__(self, output_dir="./_posts"):
        self.output_dir = output_dir
        self.ensure_directory()
    
    def ensure_directory(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def save_markdown_file(self, filename, content):
        """
        ä¿å­˜Markdownæ–‡ä»¶
        
        Args:
            filename (str): æ–‡ä»¶å
            content (str): æ–‡ä»¶å†…å®¹
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        # ç¡®ä¿æ–‡ä»¶åä»¥.mdç»“å°¾
        if not filename.endswith('.md'):
            filename += '.md'
            
        filepath = os.path.join(self.output_dir, filename)
        
        # å¤„ç†æ–‡ä»¶åå†²çª
        counter = 1
        original_filepath = filepath
        while os.path.exists(filepath):
            base, ext = os.path.splitext(original_filepath)
            filepath = f"{base}-{counter}{ext}"
            counter += 1
            
        try:
            # ä½¿ç”¨åŸå­å†™å…¥æ¨¡å¼ï¼ˆå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½åï¼‰
            temp_filepath = filepath + '.tmp'
            with open(temp_filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # é‡å‘½å
            os.replace(temp_filepath, filepath)
            
            logging.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath}")
            return filepath
        except IOError as e:
            logging.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {filepath} - {e}")
            return None


# ==========================================
# 11. Sitemapç”Ÿæˆå™¨ (è¾…åŠ©åŠŸèƒ½)
# ==========================================
class SitemapGenerator:
    """ç”Ÿæˆç®€å•çš„Sitemapç”¨äºSEO"""

    def __init__(self, output_dir, base_url="https://yourblog.com"):
        self.output_dir = output_dir
        self.base_url = base_url.rstrip('/')

    def update_sitemap(self):
        """æ‰«æ_postsç›®å½•å¹¶æ›´æ–°sitemap.xml"""
        posts_dir = os.path.join(self.output_dir, "_posts")
        if not os.path.exists(posts_dir):
            return

        urls = []
        # åŒ¹é… Jekyll æ–‡ä»¶åæ ¼å¼ YYYY-MM-DD-title.md
        pattern = re.compile(r'^(\d{4}-\d{2}-\d{2})-(.+)\.md$')
        
        for filename in os.listdir(posts_dir):
            match = pattern.match(filename)
            if match:
                date_str = match.group(1)
                slug = match.group(2)
                # Jekyll URLæ ¼å¼é€šå¸¸æ˜¯ /year/month/day/slug.html
                url_path = f"/{date_str.replace('-', '/')}/{slug}.html"
                urls.append(f"{self.base_url}{url_path}")
        
        sitemap_content = '\n'
        sitemap_content += '\n'
        for url in urls:
            sitemap_content += f'  \n    {url}\n  \n'
        sitemap_content += ''
        
        sitemap_path = os.path.join(self.output_dir, "sitemap.xml")
        try:
            with open(sitemap_path, 'w', encoding='utf-8') as f:
                f.write(sitemap_content)
            logging.info(f"Sitemapå·²æ›´æ–°: {sitemap_path}")
        except Exception as e:
            logging.error(f"æ›´æ–°Sitemapå¤±è´¥: {e}")


# ==========================================
# 12. ä¸»çˆ¬è™«ç±»
# ==========================================
class AINewsCrawler:
    """AIæ–°é—»çˆ¬è™«ä¸»ç±»ï¼Œåè°ƒæ‰€æœ‰ç»„ä»¶"""

    def __init__(self, config_manager):
        """
        åˆå§‹åŒ–AIæ–°é—»çˆ¬è™«
        
        Args:
            config_manager (ConfigManager): é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config_manager = config_manager
        self.crawler_config = config_manager.get_crawler_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.rss_parser = RSSParser(
            days_back=self.crawler_config['days_back'],
            timeout=self.crawler_config.get('timeout', 20)
        )
        
        self.content_downloader = ContentDownloader(
            timeout=self.crawler_config.get('timeout', 10),
            retries=self.crawler_config.get('retries', 3),
            user_agent=self.crawler_config.get('user_agent')
        )
        
        ali_config = config_manager.get_ali_baillan_config()
        self.ali_client = AliBaillanAPIClient(
            ali_config['api_key'],
            ali_config['endpoint']
        )
        
        self.markdown_gen = MarkdownGenerator(output_dir=self.crawler_config['output_dir'])
        self.dupe_checker = DuplicateChecker()
        self.file_manager = FileManager(output_dir=self.crawler_config['output_dir'])
        
        # é™æµå™¨
        self.rate_limiter = RateLimiter(min_interval=self.crawler_config.get('request_delay', 1.0))
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_fetched': 0,
            'downloaded': 0,
            'summarized': 0,
            'saved': 0,
            'skipped_duplicate': 0,
            'skipped_error': 0
        }
        
        # æ³¨å†Œä¿¡å·å¤„ç†ï¼Œä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        self._running = True

    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        logging.info(f"æ¥æ”¶åˆ°é€€å‡ºä¿¡å· {signum}ï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...")
        self._running = False

    def process_article(self, article):
        """
        å¤„ç†å•ç¯‡æ–‡ç« ï¼šä¸‹è½½ã€æ‘˜è¦ã€ä¿å­˜
        
        Args:
            article (dict): æ–‡ç« æ•°æ®
            
        Returns:
            bool: æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        if not self._running:
            return False
            
        # 1. æ£€æŸ¥å»é‡
        if self.dupe_checker.is_duplicate(article):
            logging.info(f"è·³è¿‡é‡å¤æ–‡ç« : {article['title']}")
            self.stats['skipped_duplicate'] += 1
            return False
        
        # 2. ä¸‹è½½å®Œæ•´å†…å®¹
        # å¦‚æœRSSä¸­å·²ç»æœ‰å®Œæ•´å†…å®¹ä¸”è¶³å¤Ÿé•¿ï¼Œå¯ä»¥è·³è¿‡ä¸‹è½½ï¼ˆå¯é€‰é€»è¾‘ï¼‰
        if not article.get('raw_content') or len(article['raw_content']) < 200:
            logging.info(f"ä¸‹è½½å†…å®¹: {article['title']}")
            article['content'] = self.content_downloader.download_content(article['link'])
            self.rate_limiter.wait() # é™æµ
        else:
            article['content'] = TextNormalizer.clean_html(article['raw_content'])
            
        if not article['content']:
            logging.warning(f"æ— æ³•è·å–å†…å®¹ï¼Œè·³è¿‡: {article['title']}")
            self.stats['skipped_error'] += 1
            return False
            
        self.stats['downloaded'] += 1
        
        # 3. ç”ŸæˆAIæ‘˜è¦
        logging.info(f"ç”Ÿæˆæ‘˜è¦: {article['title']}")
        ai_summary = self.ali_client.generate_summary(
            article['content'],
            article['title']
        )
        
        if not ai_summary or "å¤±è´¥" in ai_summary:
            logging.warning(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ‘˜è¦: {article['title']}")
            ai_summary = article.get('summary', 'æš‚æ— æ‘˜è¦')[:200]
        
        self.stats['summarized'] += 1
        
        # 4. ç”Ÿæˆæ ‡ç­¾
        tags = self._generate_tags(article, ai_summary)
        article['tags'] = tags
        
        # 5. ç”ŸæˆMarkdownå¹¶ä¿å­˜
        try:
            markdown_content = self.markdown_gen.generate_markdown_content(
                article_data=article,
                ai_summary=ai_summary,
                original_content=article['content']
            )
            
            filename = self.markdown_gen.generate_filename(
                article['title'],
                article['published']
            )
            
            saved_path = self.file_manager.save_markdown_file(filename, markdown_content)
            
            if saved_path:
                self.dupe_checker.mark_as_processed(article)
                self.stats['saved'] += 1
                return True
            else:
                return False
                
        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ç« ä¿å­˜æ—¶å‡ºé”™: {article['title']} - {e}")
            self.stats['skipped_error'] += 1
            return False

    def _generate_tags(self, article, ai_summary):
        """æ ¹æ®å†…å®¹ç”Ÿæˆæ ‡ç­¾"""
        tags = article.get('tags', []) # RSSè‡ªå¸¦çš„æ ‡ç­¾
        
        # ç¡®ä¿æœ‰åŸºç¡€æ ‡ç­¾
        if "AI" not in tags:
            tags.append("AI")
            
        content = (article['title'] + " " + (article.get('summary', '')) + " " + ai_summary).lower()
        
        # å…³é”®è¯æ˜ å°„
        keywords_map = {
            'machine learning': 'æœºå™¨å­¦ä¹ ',
            'deep learning': 'æ·±åº¦å­¦ä¹ ',
            'llm': 'å¤§æ¨¡å‹',
            'gpt': 'GPT',
            'transformer': 'Transformer',
            'nlp': 'è‡ªç„¶è¯­è¨€å¤„ç†',
            'computer vision': 'è®¡ç®—æœºè§†è§‰',
            'reinforcement learning': 'å¼ºåŒ–å­¦ä¹ ',
            'robot': 'æœºå™¨äºº',
            'generative': 'ç”Ÿæˆå¼AI',
            'openai': 'OpenAI',
            'google': 'Google',
            'deepmind': 'DeepMind',
            'arxiv': 'ArXiv',
            'chatgpt': 'ChatGPT',
            'diffusion': 'æ‰©æ•£æ¨¡å‹',
            'multimodal': 'å¤šæ¨¡æ€',
            'agent': 'æ™ºèƒ½ä½“'
        }
        
        for keyword, tag in keywords_map.items():
            if keyword in content and tag not in tags:
                tags.append(tag)
        
        # é™åˆ¶æ ‡ç­¾æ•°é‡
        return tags[:8]

    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»æµç¨‹"""
        logging.info("=" * 50)
        logging.info("AI News Crawler å¯åŠ¨")
        logging.info(f"é…ç½®: æœ€å¤§æ–‡ç« æ•°={self.crawler_config['max_articles']}, "
                    f"å¤©æ•°èŒƒå›´={self.crawler_config['days_back']}, "
                    f"è¾“å‡ºç›®å½•={self.crawler_config['output_dir']}")
        logging.info("=" * 50)
        
        rss_sources = self.config_manager.get_rss_sources()
        
        for source in rss_sources:
            if not self._running:
                logging.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢å¤„ç†æ–°çš„RSSæºã€‚")
                break
                
            if self.stats['saved'] >= self.crawler_config['max_articles']:
                logging.info(f"å·²è¾¾åˆ°æœ€å¤§æ–‡ç« ä¿å­˜æ•°é‡é™åˆ¶: {self.crawler_config['max_articles']}")
                break
            
            logging.info(f"å¼€å§‹å¤„ç†æº: {source['name']}")
            
            # è§£æRSS
            articles = self.rss_parser.parse_feed(
                source['url'],
                source['name'],
                source.get('type', 'news')
            )
            
            self.stats['total_fetched'] += len(articles)
            
            # å¤„ç†æ¯ç¯‡æ–‡ç« 
            for article in articles:
                if not self._running:
                    break
                    
                if self.stats['saved'] >= self.crawler_config['max_articles']:
                    break
                
                self.process_article(article)
                
                # æ–‡ç« é—´å»¶è¿Ÿ
                self.rate_limiter.wait()
            
            # æºé—´å»¶è¿Ÿ
            time.sleep(2)
        
        # ç»“æŸç»Ÿè®¡
        logging.info("=" * 50)
        logging.info("çˆ¬è™«ä»»åŠ¡å®Œæˆ!")
        logging.info(f"ç»Ÿè®¡ä¿¡æ¯:")
        logging.info(f"  - è·å–æ–‡ç« æ€»æ•°: {self.stats['total_fetched']}")
        logging.info(f"  - æˆåŠŸä¸‹è½½å†…å®¹: {self.stats['downloaded']}")
        logging.info(f"  - æˆåŠŸç”Ÿæˆæ‘˜è¦: {self.stats['summarized']}")
        logging.info(f"  - æˆåŠŸä¿å­˜æ–‡ç« : {self.stats['saved']}")
        logging.info(f"  - è·³è¿‡é‡å¤æ–‡ç« : {self.stats['skipped_duplicate']}")
        logging.info(f"  - è·³è¿‡é”™è¯¯æ–‡ç« : {self.stats['skipped_error']}")
        logging.info("=" * 50)
        
        # å¯é€‰ï¼šæ›´æ–°Sitemap
        # sitemap_gen = SitemapGenerator(self.crawler_config['output_dir'])
        # sitemap_gen.update_sitemap()


# ==========================================
# 13. ä¸»å…¥å£
# ==========================================
def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='AI News Crawler - è‡ªåŠ¨åŒ–AIé¢†åŸŸæ–°é—»èšåˆä¸Jekyllæ–‡ç« ç”Ÿæˆå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python ai_news_crawler.py --config config.yaml
  python ai_news_crawler.py -c config.yaml -l DEBUG
        """
    )
    
    parser.add_argument('--config', '-c', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæ ¼å¼)')
    parser.add_argument('--log-level', '-l', default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='è®¾ç½®æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    LoggerManager.setup_logging(log_level=args.log_level)
    
    try:
        # åŠ è½½é…ç½®
        logging.info(f"æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        config_manager = ConfigManager(args.config)
        
        # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
        crawler = AINewsCrawler(config_manager)
        crawler.run()
        
        logging.info("ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼Œé€€å‡ºç  0")
        sys.exit(0)
        
    except FileNotFoundError as e:
        logging.error(f"é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        sys.exit(1)
    except ValueError as e:
        logging.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        logging.info("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        logging.error(f"ç¨‹åºå‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
