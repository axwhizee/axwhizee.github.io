# -*- coding: utf-8 -*-
"""
AI é¢†åŸŸå‘¨æŠ¥ç”Ÿæˆå™¨

åŠŸèƒ½æ¦‚è¿°ï¼š
1. ä»å¤šä¸ªé¡¶çº§ AI æœºæ„çš„ RSS æºæŠ“å–æœ€è¿‘ 7 å¤©å†…çš„æ–‡ç« ï¼›
2. æå–æ¯ç¯‡æ–‡ç« çš„æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦å’Œæ¥æºï¼›
3. å°†æ±‡æ€»ä¿¡æ¯é€šè¿‡é˜¿é‡Œäº‘ç™¾ç‚¼ï¼ˆDashScopeï¼‰çš„ Qwen-Max æ¨¡å‹ï¼ˆå…¼å®¹ OpenAI APIï¼‰ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ Markdown å‘¨æŠ¥ï¼›
4. å°†æœ€ç»ˆæŠ¥å‘Šå†™å…¥æœ¬åœ°æ–‡ä»¶ `AI_Weekly_Report.md`ã€‚

ä¾èµ–åº“è¯´æ˜ï¼š
- feedparserï¼šè§£æ RSS/Atom è®¢é˜…æº
- requestsï¼šå‘èµ· HTTP è¯·æ±‚è·å– RSS å†…å®¹
- openaiï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ DashScope çš„å¤§æ¨¡å‹
- datetime / email.utilsï¼šå¤„ç†ä¸åŒæ ¼å¼çš„å‘å¸ƒæ—¶é—´
"""

import feedparser
import requests
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_tz, mktime_tz  # ç”¨äºè§£æ RFC 2822 æ ¼å¼çš„æ—¥æœŸï¼ˆå¸¸è§äº RSSï¼‰
from openai import OpenAI
import os

DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY") # ç¯å¢ƒå˜é‡ä¸­è¯»å–DashScope APIå¯†é’¥
# å®šä¹‰è¦ç›‘æ§çš„ AI é¢†åŸŸæƒå¨åšå®¢ RSS åœ°å€åˆ—è¡¨
RSS_SOURCE = [
    "https://research.google/blog/rss/",        # Google AI Blog
    "https://openai.com/news/rss.xml",          # OpenAI Blog
    "https://deepmind.com/blog/feed/",          # DeepMind Blog
    "https://huggingface.co/blog/feed.xml",     # Hugging Face Blog
    "https://bair.berkeley.edu/blog/feed.xml"   # BAIR (Berkeley AI Research)
]
MODULE = "deepseek-v3.1"
# è®¡ç®—â€œ7å¤©å‰â€çš„ UTC æ—¶é—´ç‚¹ï¼Œç”¨äºè¿‡æ»¤è¿‘æœŸæ–‡ç« 
SEVEN_DAYS_AGO = datetime.now(timezone.utc) - timedelta(days=7)

def parse_rss_date(date_str):
    """å°è¯•å°† RSS ä¸­çš„æ—¥æœŸå­—ç¬¦ä¸²è§£æä¸ºæ ‡å‡†çš„ datetime å¯¹è±¡ï¼ˆæ— æ—¶åŒºä¿¡æ¯ï¼Œä½†æŒ‰ UTC å¤„ç†ï¼‰ã€‚
    
    RSS ä¸­çš„æ—¥æœŸæ ¼å¼ä¸ç»Ÿä¸€ï¼Œå¯èƒ½ä¸ºï¼š
    - RFC 2822 æ ¼å¼ï¼ˆå¦‚ "Mon, 01 Jan 2024 12:00:00 GMT"ï¼‰
    - ISO 8601 æ ¼å¼ï¼ˆå¦‚ "2024-01-01T12:00:00Z" æˆ–å¸¦æ—¶åŒºåç§»ï¼‰
    
    å‚æ•°:
        date_str (str): åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¯èƒ½æ¥è‡ª entry.published æˆ– entry.updatedï¼‰
    
    è¿”å›:
        datetime | None: æˆåŠŸè§£æåˆ™è¿”å› naive datetimeï¼ˆè§†ä¸º UTCï¼‰ï¼Œå¦åˆ™è¿”å› None
    """
    if not date_str:
        return None

    # é¦–å…ˆå°è¯•ä½¿ç”¨ email.utils è§£æ RFC 2822 æ ¼å¼ï¼ˆæœ€å¸¸è§äº RSSï¼‰
    try:
        parsed_tuple = parsedate_tz(date_str)
        if parsed_tuple:
            timestamp = mktime_tz(parsed_tuple)  # è½¬ä¸º Unix æ—¶é—´æˆ³ï¼ˆè€ƒè™‘æ—¶åŒºï¼‰
            dt = datetime.fromtimestamp(timestamp, tz=timezone.utc)
            return dt.replace(tzinfo=None)  # è¿”å›æ— æ—¶åŒºå¯¹è±¡ï¼Œä½†å†…å®¹æ˜¯ UTC æ—¶é—´
    except Exception:
        pass  # è‹¥å¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æ ¼å¼

    # è‹¥ä¸Šè¿°å¤±è´¥ï¼Œå°è¯•å‡ ç§å¸¸è§çš„ ISO æ ¼å¼
    for fmt in ["%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S%z"]:
        try:
            dt = datetime.strptime(date_str, fmt)
            # å¦‚æœè§£æç»“æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œåˆ™é»˜è®¤ä¸º UTC
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            # è½¬æ¢ä¸º UTC å¹¶å»é™¤æ—¶åŒºä¿¡æ¯ï¼ˆä¿æŒä¸ä¸Šä¸€åˆ†æ”¯ä¸€è‡´ï¼‰
            return dt.astimezone(timezone.utc).replace(tzinfo=None)
        except ValueError:
            continue  # æ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•ä¸‹ä¸€ä¸ª

    # æ‰€æœ‰å°è¯•å‡å¤±è´¥
    return None

def fetch_rss_feed_entries(rss_source):
    """ä»æŒ‡å®šçš„RSSæºæŠ“å–æ–‡ç« æ¡ç›®åˆ—è¡¨"""
    articles = []
    for url in rss_source:
        try:
            print(f"Fetching: {url}")
            # å‘èµ· GET è¯·æ±‚è·å– RSS å†…å®¹ï¼Œè®¾ç½®è¶…æ—¶é˜²æ­¢å¡æ­»
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()  # è‹¥çŠ¶æ€ç é 2xx åˆ™æŠ›å‡ºå¼‚å¸¸
            # ä½¿ç”¨ feedparser è§£æ RSS å†…å®¹
            feed = feedparser.parse(resp.content)
            # éå†æ¯ç¯‡æ–‡ç« ï¼ˆentryï¼‰
            for entry in feed.entries:
                # ä¼˜å…ˆä½¿ç”¨ 'published'ï¼Œè‹¥æ— åˆ™å°è¯• 'updated'
                date_str = getattr(entry, 'published', None) or getattr(entry, 'updated', None)
                pub_date = parse_rss_date(date_str) if date_str else None
                # ä»…ä¿ç•™è¿‡å» 7 å¤©å†…çš„æ–‡ç« ï¼ˆæ³¨æ„ï¼šSEVEN_DAYS_AGO æ˜¯å¸¦æ—¶åŒºçš„ï¼Œéœ€å¯¹é½ï¼‰
                if pub_date and pub_date >= SEVEN_DAYS_AGO.replace(tzinfo=None):
                    title = getattr(entry, 'title', 'No Title').strip()
                    link = getattr(entry, 'link', '').strip()
                    # è·å–æ‘˜è¦å¹¶åšç®€å•æ¸…æ´—ï¼šå»æ¢è¡Œã€æˆªæ–­
                    summary = getattr(entry, 'summary', '').strip().replace('\n', ' ')[:800]
                    source = getattr(feed.feed, 'title', 'Unknown Source').strip()
                    articles.append({
                        'title': title,
                        'link': link,
                        'summary': summary,
                        'source': source,
                        'published': pub_date.isoformat()  # è½¬ä¸º ISO å­—ç¬¦ä¸²ä¾¿äºæ’åºå’Œè¾“å‡º
                    })
                    
        except Exception as e:
            # æ•è·ä»»æ„å¼‚å¸¸ï¼ˆç½‘ç»œé”™è¯¯ã€è§£æå¤±è´¥ç­‰ï¼‰ï¼Œè®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­æ•´ä½“æµç¨‹
            print(f"âš ï¸ Error fetching {url}: {e}")
            continue

    # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—ï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    articles.sort(key=lambda x: x['published'], reverse=True)
    return articles

def generate_weekly_report(articles) -> tuple[str , int]:
    """è°ƒç”¨ DashScope çš„ Qwen-Max æ¨¡å‹ï¼ˆé€šè¿‡ OpenAI å…¼å®¹ APIï¼‰ç”Ÿæˆ AI å‘¨æŠ¥ã€‚"""
    if not articles:
        raise ValueError("# AIé¢†åŸŸæœ€æ–°è¿›å±•å‘¨æŠ¥\n\næœ¬å‘¨æ— æ–°å‘å¸ƒå†…å®¹ã€‚")
    # æ„å»ºæä¾›ç»™å¤§æ¨¡å‹çš„åŸå§‹ä¸Šä¸‹æ–‡
    content = "ä»¥ä¸‹æ˜¯è¿‡å»ä¸€å‘¨æ¥è‡ª Google AIã€OpenAIã€DeepMindã€Hugging Face å’Œ BAIR ç­‰é¡¶çº§ AI æœºæ„çš„æœ€æ–°æ–‡ç« æ‘˜è¦ï¼š\n\n"
    for art in articles:
        content += f"- **{art['title']}** ï¼ˆæ¥æºï¼š{art['source']}ï¼‰\n"
        if art['summary']:
            content += f"  æ‘˜è¦ï¼š{art['summary']}\n"
        content += f"  é“¾æ¥ï¼š{art['link']}\n\n"
    # out_put(content)
    # æ„é€ æç¤ºè¯ï¼ˆPromptï¼‰ï¼Œæ˜ç¡®è¦æ±‚æ¨¡å‹è¾“å‡ºç»“æ„åŒ–ã€æœ‰æ´å¯ŸåŠ›çš„åˆ†æ
    prompt = f"""è¯·åŸºäºä»¥ä¸‹è¿‘æœŸ AI é¢†åŸŸçš„æŠ€æœ¯åšå®¢æ‘˜è¦ï¼ˆå«é“¾æ¥ï¼‰ï¼Œæ’°å†™ä¸€ä»½åä¸ºã€ŠAIé¢†åŸŸæœ€æ–°è¿›å±•å‘¨æŠ¥ã€‹çš„æŠ¥å‘Šï¼Œè¦æ±‚ï¼š\n
1. æŠ¥å‘Šå¿…é¡»ä½¿ç”¨ Markdown æ ¼å¼ï¼›
2. æ ‡é¢˜ä¸ºï¼š**AIé¢†åŸŸæœ€æ–°è¿›å±•å‘¨æŠ¥**ï¼›
3. å†…å®¹éœ€åŒ…å«ï¼š
   - å½“å‰ä¸»è¦å‘å±•æ–¹å‘ï¼ˆå¦‚å¤šæ¨¡æ€ã€æ¨ç†èƒ½åŠ›ã€å¼€æºç”Ÿæ€ã€å…·èº«æ™ºèƒ½ç­‰ï¼‰ï¼›
   - æœ¬å‘¨çªå‡ºçš„æµè¡Œäº§å“æˆ–æŠ€æœ¯ï¼ˆå¦‚æ–°æ¨¡å‹ã€æ¡†æ¶ã€å·¥å…·ï¼‰ï¼›
\n{content}\n"""
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ŒæŒ‡å‘ DashScope çš„å…¼å®¹ API ç«¯ç‚¹
    client = OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    # è°ƒç”¨ Qwen-Max æ¨¡å‹ç”ŸæˆæŠ¥å‘Š
    response = client.chat.completions.create(
        model=MODULE,
        messages=[
            {
                'role': 'system',
                'content': (
                    'ä½ æ˜¯ä¸€ä½èµ„æ·± AI è¡Œä¸šåˆ†æå¸ˆï¼Œä½ çš„æªè¾åº”å½“ï¼šä¸“ä¸šã€ç®€æ´ã€æœ‰æ´å¯ŸåŠ›ï¼Œé¿å…ç½—åˆ—ï¼Œ'
                    'ä¸”å°¤å…¶æ³¨æ„ï¼šä¸è¦è™šæ„æœªæåŠçš„å†…å®¹ï¼Œä»…åŸºäºæ‰€æä¾›ä¸æœç´¢åˆ°çš„èµ„æ–™ä¿¡æ¯æ¨ç†ã€‚'
                )
            },
            {'role': 'user', 'content': prompt}
        ],
        temperature=0.3,      # è¾ƒä½æ¸©åº¦ä»¥ä¿è¯è¾“å‡ºç¨³å®šæ€§å’Œäº‹å®æ€§
        max_tokens=3000       # å…è®¸ç”Ÿæˆè¾ƒé•¿æŠ¥å‘Š
    )
    content = response.choices[0].message.content
    if content is None:
        raise ValueError("å“åº”ä¸ºç©º")
    try:
        if response.usage is None:  # ç¡®ä¿usageä¸ä¸ºNoneï¼Œä½†å®¹è®¸å¦‚æ­¤
            token_usage = 0
            raise Exception("Usage info missing")
        token_usage = response.usage.total_tokens
    except:
        print("è­¦å‘Šï¼šæ— æ³•è§£ætokenä½¿ç”¨é‡\n")
        token_usage = 0
    return content, token_usage

def out_put(content: str, file_path: str = "./output.md"):
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
    except Exception as e:
        print(f"å¯¼å‡ºæ–‡ä»¶{file_path}å¤±è´¥ï¼{e}\n")
    finally:
        print(f"æ–‡ä»¶{file_path}å¯¼å‡ºæˆåŠŸï¼\n")

if __name__ == "__main__":
    print("æ­£åœ¨æŠ“å–æœ€è¿‘7å¤©çš„AIå‰æ²¿æ–‡ç« ...")
    articles = fetch_rss_feed_entries(RSS_SOURCE)
    print(f"âœ… å…±è·å– {len(articles)} ç¯‡æ–°æ–‡ç« ã€‚")
    print(f"æ­£åœ¨è°ƒç”¨{MODULE}ï¼ˆé€šè¿‡ OpenAI å…¼å®¹æ¥å£ï¼‰ç”Ÿæˆå‘¨æŠ¥...")
    report, token_usage = generate_weekly_report(articles)

    # è·å–æ—¥æœŸä»¥ä¾¿ç”Ÿæˆæ–‡æ¡£
    date = datetime.now().date()
    # å°†æŠ¥å‘Šå†™å…¥æ–‡ä»¶ï¼Œå¤´éƒ¨æ·»åŠ  Front Matterï¼ˆé€‚ç”¨äºé™æ€åšå®¢å¦‚ Hugoï¼‰
    out_put(f"""---\ntitle: "AI Weekly Report({date})"\ndate: {date}\n---\n{report}""",
            f"./_posts/{date}-Post.md")
    print(f"ğŸ“„ å‘¨æŠ¥ç”ŸæˆæˆåŠŸï¼\nä½¿ç”¨Tokenï¼š{token_usage}")

"""æ—¥å¿—
æ­£åœ¨æŠ“å–æœ€è¿‘7å¤©çš„AIå‰æ²¿æ–‡ç« ...
Fetching: https://research.google/blog/rss/
Fetching: https://openai.com/news/rss.xml
Fetching: https://deepmind.com/blog/feed/
Fetching: https://huggingface.co/blog/feed.xml
Fetching: https://bair.berkeley.edu/blog/feed.xml
âœ… å…±è·å– 11 ç¯‡æ–°æ–‡ç« ã€‚
æ­£åœ¨è°ƒç”¨ Qwen-Maxï¼ˆé€šè¿‡ OpenAI å…¼å®¹æ¥å£ï¼‰ç”Ÿæˆå‘¨æŠ¥...
ğŸ“„ å‘¨æŠ¥ç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜è‡³: AI_Weekly_Reporter.md
ä½¿ç”¨Tokenï¼š1566
"""
